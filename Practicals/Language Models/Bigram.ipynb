{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIHEdCOQN-r-"
      },
      "outputs": [],
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"Tokenise a line\"\"\"\n",
        "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
        "    return ['<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "EMBEDDING_DIM = 4\n",
        "CONTEXT_SIZE = 1 #!!!#\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# Bigram Neural Network Model\n",
        "class BigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(BigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "training_samples = []\n",
        "vocabulary = set(['<UNK>'])\n",
        "file1 = open('train.txt', 'r')\n",
        "lines = file1.readlines()\n",
        "#lines = \n",
        "print(lines)\n",
        "for line in lines:\n",
        "    tokens = preprocess(line)\n",
        "    for i in tokens: vocabulary.add(i) \n",
        "    training_samples.append(tokens)\n",
        "    print(line)\n",
        "    #line = sys.stdin.readline()\n",
        "\n",
        "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for tokens in training_samples:\n",
        "    for i in range(len(tokens) - 1): #!!!#\n",
        "        x_train.append([word2idx[tokens[i]]]) #!!!#\n",
        "        y_train.append([word2idx[tokens[i+1]]]) #!!!#\n",
        "\n",
        "print(\"x_train\",x_train)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:1] #!!!#\n",
        "        target_tensor = data_tensor[:,1] #!!!#\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()    \n",
        "\n",
        "    print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
        "\n",
        "print('Model saved.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM3DLRL7OE5x",
        "outputId": "2fb77317-0b45-4a63-b361-2ebde47235c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are you still here?\\n', 'where are you?\\n', 'are you tired?\\n', 'i am tired.\\n', 'are you in england?\\n', 'were you in mexico?\\n']\n",
            "are you still here?\n",
            "\n",
            "where are you?\n",
            "\n",
            "are you tired?\n",
            "\n",
            "i am tired.\n",
            "\n",
            "are you in england?\n",
            "\n",
            "were you in mexico?\n",
            "\n",
            "x_train [[0], [11], [12], [1], [7], [0], [8], [11], [12], [0], [11], [12], [15], [0], [9], [5], [15], [0], [11], [12], [14], [3], [0], [10], [12], [14], [4]]\n",
            "Epoch: 0 loss: 2.4375405311584473\n",
            "Epoch: 1 loss: 2.2202789783477783\n",
            "Epoch: 2 loss: 1.9671860933303833\n",
            "Epoch: 3 loss: 1.6362926959991455\n",
            "Epoch: 4 loss: 1.2517435550689697\n",
            "Epoch: 5 loss: 0.9026921987533569\n",
            "Epoch: 6 loss: 0.656467080116272\n",
            "Epoch: 7 loss: 0.5051158666610718\n",
            "Epoch: 8 loss: 0.4132114350795746\n",
            "Epoch: 9 loss: 0.3550700843334198\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "blob = torch.load('model.lm')\n",
        "idx2word = blob['vocab']\n",
        "word2idx = {k: v for v, k in idx2word.items()}\n",
        "vocabulary = set(idx2word.values())\n",
        "\n",
        "model = BigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "model.load_state_dict(blob['model'])\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "file1 = open('test.txt', 'r')\n",
        "lines = file1.readlines()\n",
        "#lines = sys.stdin.readline()\n",
        "for line in lines:\n",
        "    tokens = preprocess(line)\n",
        "    \n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for i in range(len(tokens) - 1): #!!!#\n",
        "        x_test.append([word2idx[tokens[i]]]) #!!!#\n",
        "        y_test.append([word2idx[tokens[i+1]]]) #!!!#\n",
        "    \n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    total_prob = 1.0\n",
        "    for i, data_tensor in enumerate(test_loader):\n",
        "        context_tensor = data_tensor[:,0:1] #!!!#\n",
        "        target_tensor = data_tensor[:,1] #!!!#\n",
        "        log_probs = model(context_tensor)\n",
        "        probs = torch.exp(log_probs)\n",
        "        predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "    \n",
        "        true_label = y_test[i][0]\n",
        "        true_word = idx2word[true_label]\n",
        "    \n",
        "        prob_true = float(probs[0][true_label])\n",
        "        total_prob *= prob_true\n",
        "    \n",
        "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
        "    \n",
        "    #line = sys.stdin.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc4Be08nOPzL",
        "outputId": "a5cf59e1-ea0c-40c0-c41a-fd1c78387dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.006567\t-5.025658\t ['<BOS>', 'where', 'are', 'you', '?']\n",
            "0.002403\t-6.030873\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n",
            "0.026050\t-3.647736\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000067\t-9.614363\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.000672\t-7.304865\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    }
  ]
}