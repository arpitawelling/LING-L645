{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_b1hBlGS6-M"
      },
      "outputs": [],
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "def preprocess(s):\n",
        "    \"\"\"Tokenise a line\"\"\"\n",
        "    o = re.sub('([^a-zA-Z0-9\\']+)', ' \\g<1> ', s.strip())\n",
        "    return ['<BOS>'] + re.sub('  *', ' ', o).strip().split(' ')\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "EMBEDDING_DIM = 4\n",
        "CONTEXT_SIZE = 2 #!!!#\n",
        "HIDDEN_DIM = 6\n",
        "\n",
        "# Bigram Neural Network Model\n",
        "class TrigramNNmodel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
        "        super(TrigramNNmodel, self).__init__()\n",
        "        self.context_size = context_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, vocab_size, bias = False)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # compute x': concatenation of x1 and x2 embeddings\n",
        "        embeds = self.embeddings(inputs).view(\n",
        "                (-1,self.context_size * self.embedding_dim))\n",
        "        # compute h: tanh(W_1.x' + b)\n",
        "        out = torch.tanh(self.linear1(embeds))\n",
        "        # compute W_2.h\n",
        "        out = self.linear2(out)\n",
        "        # compute y: log_softmax(W_2.h)\n",
        "        log_probs = F.log_softmax(out, dim=1)\n",
        "        # return log probabilities\n",
        "        # BATCH_SIZE x len(vocab)\n",
        "        return log_probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "training_samples = []\n",
        "vocabulary = set(['<UNK>'])\n",
        "file1 = open('train.txt', 'r')\n",
        "lines = file1.readlines()\n",
        "#lines = \n",
        "print(lines)\n",
        "for line in lines:\n",
        "    tokens = preprocess(line)\n",
        "    for i in tokens: vocabulary.add(i) \n",
        "    training_samples.append(tokens)\n",
        "    print(line)\n",
        "    #line = sys.stdin.readline()\n",
        "\n",
        "word2idx = {k: v for v, k in enumerate(vocabulary)}\n",
        "idx2word = {v: k for k, v in word2idx.items()}\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "for tokens in training_samples:\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_train.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        print(\"tokens[i]\",tokens[i])\n",
        "        print(\"tokens[i+1]\",tokens[i+1])\n",
        "        y_train.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "\n",
        "print(x_train)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "NUM_EPOCHS = 20\n",
        "\n",
        "train_set = np.concatenate((x_train, y_train), axis=1)\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "loss_function = nn.NLLLoss()\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    for i, data_tensor in enumerate(train_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        log_probs = model(context_tensor)\n",
        "        loss = loss_function(log_probs, target_tensor)\n",
        "\n",
        "        loss.backward()\n",
        "        optimiser.step()    \n",
        "\n",
        "    print('Epoch:', epoch, 'loss:', float(loss))\n",
        "\n",
        "torch.save({'model': model.state_dict(), 'vocab': idx2word}, 'model.lm')\n",
        "\n",
        "print('Model saved.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtoXp4CfTF4m",
        "outputId": "d7bf4c17-690f-4f87-c645-16282dd872d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are you still here?\\n', 'where are you?\\n', 'are you tired?\\n', 'i am tired.\\n', 'are you in england?\\n', 'were you in mexico?\\n']\n",
            "are you still here?\n",
            "\n",
            "where are you?\n",
            "\n",
            "are you tired?\n",
            "\n",
            "i am tired.\n",
            "\n",
            "are you in england?\n",
            "\n",
            "were you in mexico?\n",
            "\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] are\n",
            "tokens[i] are\n",
            "tokens[i+1] you\n",
            "tokens[i] you\n",
            "tokens[i+1] still\n",
            "tokens[i] still\n",
            "tokens[i+1] here\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] where\n",
            "tokens[i] where\n",
            "tokens[i+1] are\n",
            "tokens[i] are\n",
            "tokens[i+1] you\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] are\n",
            "tokens[i] are\n",
            "tokens[i+1] you\n",
            "tokens[i] you\n",
            "tokens[i+1] tired\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] i\n",
            "tokens[i] i\n",
            "tokens[i+1] am\n",
            "tokens[i] am\n",
            "tokens[i+1] tired\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] are\n",
            "tokens[i] are\n",
            "tokens[i+1] you\n",
            "tokens[i] you\n",
            "tokens[i+1] in\n",
            "tokens[i] in\n",
            "tokens[i+1] england\n",
            "tokens[i] <BOS>\n",
            "tokens[i+1] were\n",
            "tokens[i] were\n",
            "tokens[i+1] you\n",
            "tokens[i] you\n",
            "tokens[i+1] in\n",
            "tokens[i] in\n",
            "tokens[i+1] mexico\n",
            "[[11, 14], [14, 8], [8, 15], [15, 13], [11, 12], [12, 14], [14, 8], [11, 14], [14, 8], [8, 9], [11, 10], [10, 1], [1, 9], [11, 14], [14, 8], [8, 7], [7, 3], [11, 5], [5, 8], [8, 7], [7, 4]]\n",
            "Epoch: 0 loss: 2.5552539825439453\n",
            "Epoch: 1 loss: 2.2540605068206787\n",
            "Epoch: 2 loss: 1.9240334033966064\n",
            "Epoch: 3 loss: 1.5875084400177002\n",
            "Epoch: 4 loss: 1.287392258644104\n",
            "Epoch: 5 loss: 1.0346938371658325\n",
            "Epoch: 6 loss: 0.8286635875701904\n",
            "Epoch: 7 loss: 0.6670756936073303\n",
            "Epoch: 8 loss: 0.5429031252861023\n",
            "Epoch: 9 loss: 0.44639986753463745\n",
            "Epoch: 10 loss: 0.37100058794021606\n",
            "Epoch: 11 loss: 0.31303921341896057\n",
            "Epoch: 12 loss: 0.2689843475818634\n",
            "Epoch: 13 loss: 0.23522965610027313\n",
            "Epoch: 14 loss: 0.20869623124599457\n",
            "Epoch: 15 loss: 0.18701189756393433\n",
            "Epoch: 16 loss: 0.168476402759552\n",
            "Epoch: 17 loss: 0.15200553834438324\n",
            "Epoch: 18 loss: 0.1370541751384735\n",
            "Epoch: 19 loss: 0.12348466366529465\n",
            "Model saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, re\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "blob = torch.load('model.lm')\n",
        "idx2word = blob['vocab']\n",
        "word2idx = {k: v for v, k in idx2word.items()}\n",
        "vocabulary = set(idx2word.values())\n",
        "\n",
        "model = TrigramNNmodel(len(vocabulary), EMBEDDING_DIM, CONTEXT_SIZE, HIDDEN_DIM)\n",
        "model.load_state_dict(blob['model'])\n",
        "\n",
        "###############################################################################\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "file1 = open('test.txt', 'r')\n",
        "lines = file1.readlines()\n",
        "#lines = sys.stdin.readline()\n",
        "for line in lines:\n",
        "    tokens = preprocess(line)\n",
        "    \n",
        "    x_test = []\n",
        "    y_test = []\n",
        "    for i in range(len(tokens) - 2): #!!!#\n",
        "        x_test.append([word2idx[tokens[i]],word2idx[tokens[i+1]]]) #!!!#\n",
        "        y_test.append([word2idx[tokens[i+2]]]) #!!!#\n",
        "    \n",
        "    x_test = np.array(x_test)\n",
        "    y_test = np.array(y_test)\n",
        "    \n",
        "    test_set = np.concatenate((x_test, y_test), axis=1)\n",
        "    test_loader = DataLoader(test_set, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    total_prob = 1.0\n",
        "    for i, data_tensor in enumerate(test_loader):\n",
        "        context_tensor = data_tensor[:,0:2] #!!!#\n",
        "        target_tensor = data_tensor[:,2] #!!!#\n",
        "        log_probs = model(context_tensor)\n",
        "        probs = torch.exp(log_probs)\n",
        "        predicted_label = int(torch.argmax(probs, dim=1)[0])\n",
        "    \n",
        "        true_label = y_test[i][0]\n",
        "        true_word = idx2word[true_label]\n",
        "    \n",
        "        prob_true = float(probs[0][true_label])\n",
        "        total_prob *= prob_true\n",
        "    \n",
        "    print('%.6f\\t%.6f\\t' % (total_prob, math.log(total_prob)), tokens)\n",
        "    \n",
        "    #line = sys.stdin.readline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OV2Zw6PVtC3",
        "outputId": "50976d4f-7d0a-4503-aea0-5f1eb3646165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.112273\t-2.186819\t ['<BOS>', 'where', 'are', 'you', '?']\n",
            "0.176394\t-1.735033\t ['<BOS>', 'were', 'you', 'in', 'england', '?']\n",
            "0.069201\t-2.670745\t ['<BOS>', 'are', 'you', 'in', 'mexico', '?']\n",
            "0.000043\t-10.057258\t ['<BOS>', 'i', 'am', 'in', 'mexico', '.']\n",
            "0.002584\t-5.958396\t ['<BOS>', 'are', 'you', 'still', 'in', 'mexico', '?']\n"
          ]
        }
      ]
    }
  ]
}